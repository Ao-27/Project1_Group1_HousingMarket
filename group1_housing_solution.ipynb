{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "Main_csv_path = Path(\"Resources/18100205.csv\")\n",
    "Metadata_csv_path = Path(\"Resources/18100205_MetaData.csv\")\n",
    "\n",
    "Main_StatsCan_IndexDF = pd.read_csv(Main_csv_path, encoding=\"UTF-8-Sig\")\n",
    "Metadata_DF = pd.read_csv(Metadata_csv_path, encoding=\"UTF-8-Sig\")\n",
    "\n",
    "Main_StatsCan_IndexDF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a3466",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Metadata_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select necessary columns for initial data cleaning processes.\n",
    "\n",
    "reduced_main_df = Main_StatsCan_IndexDF[[\"REF_DATE\",\"GEO\",\"New housing price indexes\",\"UOM\",\"VALUE\",\"STATUS\",\"DECIMALS\"]]\t\t\n",
    "#reduced_main_df.head()\n",
    "\n",
    "reduced_main_df.columns.values[2] = \"NewHousePrice_Index_Type\" #Replaced column name of \"New housing price indexes\".\n",
    "\n",
    "reduced_main_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0240861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "#print(re.split(r',',reduced_main_df[\"GEO\"]))\n",
    "Orig_Geo_series = pd.Series(reduced_main_df[\"GEO\"])\n",
    "#print(Orig_Geo_series)\n",
    "Geo_Split = Orig_Geo_series.str.split(',')\n",
    "\n",
    "Geo_CityRow = Geo_Split.str.get(0) #access the first element for eventual \"column1\"\n",
    "\n",
    "Geo_CityRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ce3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Geo_ProvRow = Geo_Split.str.get(1) #access the first element for eventual \"column2\"\n",
    "\n",
    "Geo_ProvRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc45c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp_DF = pd.concat(reduced_main_df,Geo_City )\n",
    "\n",
    "Manual_Merge_Geo = pd.concat([Geo_CityRow,Geo_ProvRow], axis=1) #axis=1 is using \"column 1\" to merge.\n",
    "\n",
    "Manual_Merge_Geo\n",
    "\n",
    "#Rename Options presented via => https://www.geeksforgeeks.org/rename-specific-columns-in-pandas/\n",
    "Manual_Merge_Geo.columns.values[0] = \"City_Region\"\n",
    "Manual_Merge_Geo.columns.values[1] = \"Province\"\n",
    "\n",
    "Manual_Merge_Geo\n",
    "\n",
    "#BEST PRACTICE MIGHT BE TO RENAME RIGHT AWAY \"HERE\" ... to save coding time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc50dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Orig_Year_series = pd.Series(reduced_main_df[\"REF_DATE\"])\n",
    "Year_Split = Orig_Year_series.str.split('-')\n",
    "\n",
    "Row_Year = Year_Split.str.get(0) #access the first element for eventual \"column1\"\n",
    "\n",
    "Row_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605eacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Row_Month = Year_Split.str.get(1) #access the first element for eventual \"column1\"\n",
    "\n",
    "Row_Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071dd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "Manual_Merge_date = pd.concat([Row_Year,Row_Month], axis=1) #axis=1 is using \"column 1\" to merge.\n",
    "\n",
    "Manual_Merge_date\n",
    "\n",
    "#Rename Options presented via => https://www.geeksforgeeks.org/rename-specific-columns-in-pandas/\n",
    "Manual_Merge_date.columns.values[0] = \"Year\"\n",
    "Manual_Merge_date.columns.values[1] = \"Month\"\n",
    "\n",
    "Manual_Merge_date\n",
    "#BEST PRACTICE MIGHT BE TO RENAME RIGHT AWAY \"HERE\" ... to save coding time later.\n",
    "#KEY REMINDER ... if we need the month to be transformed into \"1\" intstead of \"01\" (IE: INT from TEXT/VARCHAR) ... here is probably the best place as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb061e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#via => https://appdividend.com/2020/06/02/pandas-concat-method-example-in-python/\n",
    "#JOIN Split GEO Columns to Original DF\n",
    "\n",
    "GeoUpdated_DF = pd.concat([reduced_main_df,Manual_Merge_Geo],axis=1,join='outer') # ,join_axes=None,ignore_index=False)\n",
    "\n",
    "GeoUpdated_DF\n",
    "\n",
    "#JOIN Split YEar+Month Columns to GeoUpdated_DF\n",
    "\n",
    "DateUpdated_DF = pd.concat([GeoUpdated_DF,Manual_Merge_date],axis=1,join='outer')\n",
    "\n",
    "DateUpdated_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_main_df = Main_StatsCan_IndexDF[[\"REF_DATE\",\"GEO\",\"New housing price indexes\",\"UOM\",\"VALUE\",\"STATUS\",\"DECIMALS\"]]\n",
    "\n",
    "Analysis_DF = DateUpdated_DF[[\"Year\",\"Month\",\"City_Region\",\"Province\",\"NewHousePrice_Index_Type\",\"UOM\",\"VALUE\",\"STATUS\",\"DECIMALS\"]]\n",
    "\n",
    "Analysis_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01861d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove \"junk data\" columns based on StatsCan Legend.\n",
    "\n",
    "StatusClean_Analysis_DF = Analysis_DF.query(\"STATUS not in ['E', 'x', '..']\") # QUERY fuction delivers to python \"memory\" a \"true/false\" outcome.\n",
    "TotalPriceIndex_Analysis_DF = StatusClean_Analysis_DF.query(\"NewHousePrice_Index_Type not in ['House only','']\")\n",
    "#StatusClean_Analysis_DF\n",
    "\n",
    "TotalPriceIndex_Analysis_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cities = ['Toronto', 'Vancouver', 'MontrÃ©al']\n",
    "filtered_city_data = {}\n",
    "\n",
    "for city in desired_cities:\n",
    "    city_filtered_df = TotalPriceIndex_Analysis_DF[TotalPriceIndex_Analysis_DF['City_Region'] == city]\n",
    "    filtered_city_data[city] = city_filtered_df\n",
    "\n",
    "# Print the data for all cities\n",
    "for city, city_df in filtered_city_data.items():\n",
    "    print(f\"Data for {city}:\")\n",
    "    print(city_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "#Q from Ajani - 2023-07-24 - Are we confident that this is brining us all of the cities or is this the issue we discussed to look into validating? The validation was whether the code is producing all \"cities\" vs just the \"last/first\" one it finds in the list above.\n",
    "\n",
    "#Solution1 Idea - 2023-07-24 - a NumPy Array may be necessary rather than a list. **OR** we create a Dictionary from the list ... with the Key being \"City_Region\" ... similar to our custom column rename above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c91d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract sample data to validate that the STATUS not in funciton above was successful.\n",
    "Output_Folder = 'Output/'\n",
    "Output_file = Output_Folder + 'city_filtered_df.csv'\n",
    "\n",
    "# Concatenate DataFrames for all cities\n",
    "combined_data = pd.concat(filtered_city_data.values(), ignore_index=True)\n",
    "\n",
    "#Q from Ajani ... we should look into this. It doesn't make sense for ignore_index=True here in my opinion. Very open to discussion. AO - 2023-07-24\n",
    "#Q from Ajani ... are we sure all 3 cities are produced? Wondering if there's an iteration issue for Python/Pandas. Let's discuss at some point. AO - 2023-07-24\n",
    "\n",
    "# Save the combined data to a single CSV file\n",
    "combined_data.to_csv(Output_file, index=False, encoding='utf-8-sig') # ref for variation in export encoding https://stackoverflow.com/questions/57152985/what-is-the-difference-between-utf-8-and-utf-8-sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Housing_Prices_by_year = Path(\"Resources/Housing_Prices_by_year.csv\")\n",
    "Housing_Prices_by_year_df = pd.read_csv(Housing_Prices_by_year, encoding=\"UTF-8-Sig\")\n",
    "\n",
    "Housing_Prices_by_year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validating which DataTypes are within the .csv of our new Housing Price DataSet.\n",
    "\n",
    "Housing_Prices_by_year_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns for code error appearing associated to \"Vancouver\" (Likely a \"text trim\")  issue.\n",
    "\n",
    "Housing_Prices_by_year_df.columns.values[1] = \"Vancouver\"\n",
    "Housing_Prices_by_year_df.columns.values[2] = \"Toronto\"\n",
    "Housing_Prices_by_year_df.columns.values[3] = \"Montreal\"\n",
    "\n",
    "print(Housing_Prices_by_year_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a93043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validating the Structure of the data from the .csv as Date Format input from \"user\" (the csv data set) to Python's/Panda's processing needs.\n",
    "#Panda's processing needs can be observed here via URL => \n",
    "\n",
    "Housing_Prices_by_year_df['month_Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 => Goal of next step is to \"establish\" a column being similar to StatsCan original data of \"YYYY-MM\".\n",
    "#2 => We will use the following URL to establish what our \"format=\" parameter code has to be ... based on OUR USER INPUT DATA (IE: the .csv.) URL => https://www.w3schools.com/python/python_datetime.asp\n",
    "\n",
    "pd.to_datetime(Housing_Prices_by_year_df['month_Year'], format='%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "#SUCCESS ... (comment on the \"why\" to the team ... we are being \"literal coders here\" ... to produce the \"date as a parameter\" ... the params are \"Y/M/D\" ... and Python does it using the \"to_datetime functions ... parameter that is called \"format=\" ... format='%Y-%m-%d' )\n",
    "#For the \"pd.to_datetime function ... the params are as follows.... (1 = you column to be processed/iterated over ... 2 = the format WITHIN that column ... 3 = what to do when the column you've targeted \"doesn't fit\" ... int the FORMAT you confirmed is the \"data structure\" for all entries. IE: basically we've developed an \"#N/A\" type output similar to an Excel \"vlookup\" ... but for dates in PYTHON ....)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we have a \"four stepper\" as per our initial work above.\n",
    "# This block is dedicated to \"step #1\" of repeating the pd.concat() approach above.\n",
    "# pd.series() is CRITICAL for our \"expected\" output and data frame to \"join to\" ... the original \".csv\" for us to then delete the \"bad data\" ... \n",
    "#\n",
    "#Step #1 - 1> Orig_Geo_series = pd.Series(reduced_main_df[\"GEO\"]) ... sets up \"index\" to match original dataset.\n",
    "#Step #2 - 1> Geo_Split = Orig_Geo_series.str.split(',') ... splits up the \"data\" into our desired output.\n",
    "#2 - 2> Geo_CityRow = Geo_Split.str.get(0) #access the first element for eventual \"column1\" ... sets up \"variables\" ... for renaming.\n",
    "#2 - 3> Geo_CityRow ... Call's the row for \"validation\" ... \n",
    "#Step #3 - 1> DateUpdated_DF = pd.concat([GeoUpdated_DF,Manual_Merge_date],axis=1,join='outer')\n",
    "#3 - 2> DateUpdated_DF ... Call's \"merged data\" for \"validation\" ...\n",
    "#Step 4 - 1> #Manual_Merge_date.columns.values[0] = \"Year\" ... Renames the column outputs merged into new DF ... to \"desired for presentation\" naming convention.\n",
    "\n",
    "#Pseudo code \"Step 1\" ... break the \"user data\" (the .csv) column into 3 parameters ...\n",
    "\n",
    "Orig_HP_DateTime_To_Date = pd.Series(\n",
    "     pd.to_datetime(Housing_Prices_by_year_df['month_Year'], format='%Y-%m-%d', errors = 'coerce').dt.strftime('%Y-%m-%d')\n",
    ")\n",
    "\n",
    "Orig_HP_DateTime_To_Date \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is executing Step #2 above.\n",
    "\n",
    "HP_Month_Year_DF = Orig_HP_DateTime_To_Date.str.split('-')\n",
    "\n",
    "HP_Year_Row = HP_Month_Year_DF.str.get(0) #access the first element for eventual \"column1\"\n",
    "HP_Month_Row = HP_Month_Year_DF.str.get(1) #access the 2nd element for eventual \"column2\"\n",
    "HP_Day_Row = HP_Month_Year_DF.str.get(2) #access the 3rd element for eventual \"column3\"\n",
    "\n",
    "print(HP_Year_Row)\n",
    "print(HP_Month_Row)\n",
    "print(HP_Day_Row)\n",
    "\n",
    "\n",
    "#Orig_HP_DateSeries = pd.Series(\n",
    "#    pd.to_datetime(Housing_Prices_by_year_df['month_Year'], format='%Y-%m-%d', errors = 'coerce')\n",
    "#    )\n",
    "#NOTE: I split the code above just for discussion on the SYNTAX with the team (it can be set back to one line after).\n",
    "#NOTE2: the following between the \"<>\" failed ... because I need to convert to STRING ... to PARSE ... via the Python \"str.split\" function ... <HP_Split_DF =  Orig_HP_DateSeries.str.split('-')>\n",
    "#NOTE3: ACTUALLY NO ... the following is \"just a trim\" ... we DON'T WANT THAT - Eventually we want \"lstrip\" via URL => https://www.tutsmake.com/python-trim-left-or-right-string/\n",
    "#NOTE3: NEW APPROACH  via URL => https://www.statology.org/pandas-convert-datetime-to-string/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c972257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is executing Step #3 & #4 above\n",
    "\n",
    "Manual_Merge_HP_Date =  pd.concat([HP_Year_Row,HP_Month_Row,HP_Day_Row], axis=1)\n",
    "#Manual_Merge_HP_Date\n",
    "Manual_Merge_HP_Date.columns.values[0] = \"HousePrice_Year\"\n",
    "Manual_Merge_HP_Date.columns.values[1] = \"HousePrice_Month\"\n",
    "Manual_Merge_HP_Date.columns.values[2] = \"HousePrice_Day\"\n",
    "\n",
    "Manual_Merge_HP_Date\n",
    "\n",
    "#EXAMPLE Code Reference from above.\n",
    "#Manual_Merge_date = pd.concat([Row_Year,Row_Month], axis=1) #axis=1 is using \"column 1\" to merge.\n",
    "#Manual_Merge_date\n",
    "#Rename Options presented via => https://www.geeksforgeeks.org/rename-specific-columns-in-pandas/\n",
    "#Manual_Merge_date.columns.values[0] = \"Year\"\n",
    "#Manual_Merge_date.columns.values[1] = \"Month\"\n",
    "#Manual_Merge_date\n",
    "#BEST PRACTICE MIGHT BE TO RENAME RIGHT AWAY \"HERE\" ... to save coding time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9572e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is merging back into the Original .csv \"user data\" and also removing columns no longer needed.\n",
    "#Purpose = to prepare to merge to the \"master Analysis data sheet\" on a city by city basis.\n",
    "\n",
    "Updated_HP_by_Year_DF = pd.concat([Housing_Prices_by_year_df,Manual_Merge_HP_Date], axis=1,join='outer')\n",
    "#Updated_HP_by_Year_DF\n",
    "\n",
    "Van_Analysis_HP_by_Year_DF = Updated_HP_by_Year_DF[[\"HousePrice_Year\",\"HousePrice_Month\",\"HousePrice_Day\",\"Vancouver\"]] #,\"Montreal\",\"Toronto\"]]\n",
    "Van_Analysis_HP_by_Year_DF\n",
    "\n",
    "\n",
    "#Step1>EXAMPLE CODE from earlier in the Notebook:\n",
    "#GeoUpdated_DF = pd.concat([reduced_main_df,Manual_Merge_Geo],axis=1,join='outer') # ,join_axes=None,ignore_index=False)\n",
    "#GeoUpdated_DF\n",
    "#Step2>EXAMPLE CODE from earlier:\n",
    "#Analysis_DF = DateUpdated_DF[[\"Year\",\"Month\",\"City_Region\",\"Province\",\"NewHousePrice_Index_Type\",\"UOM\",\"VALUE\",\"STATUS\",\"DECIMALS\"]]\n",
    "#Analysis_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75caaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above for Montreal. KEY NOTE not cleaning this data (IE: \"nulls/NaN\") until we get into the Master Data Set.\n",
    "\n",
    "Mtl_Analysis_HP_by_Year_DF = Updated_HP_by_Year_DF[[\"HousePrice_Year\",\"HousePrice_Month\",\"HousePrice_Day\",\"Montreal\"]] #,\"Vancouver\",\"Montreal\",\"Toronto\"]]\n",
    "Mtl_Analysis_HP_by_Year_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above for Montreal. KEY NOTE not cleaning this data (IE: \"nulls/NaN\") until we get into the Master Data Set.\n",
    "\n",
    "Tor_Analysis_HP_by_Year_DF = Updated_HP_by_Year_DF[[\"HousePrice_Year\",\"HousePrice_Month\",\"HousePrice_Day\",\"Toronto\"]] #,\"Vancouver\",\"Montreal\",\"Toronto\"]]\n",
    "Tor_Analysis_HP_by_Year_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688422cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a test cell for the \"if + elif\" row ... as For (?) LOOP to populate a data frame. I'm not 100% certain the cells above are executing 100% of everything we need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941aadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET UP the basepoints for the calculation with historical prices ...\n",
    "#Create VARs to enable them to run against the cleaned data set ...\n",
    "\n",
    "#Determine how to complete the analysis ... and fit into how they can be placed into slides.\n",
    "\n",
    "#Confirm the types of \"plots\" ... we're confident in to speak to the story of the data analysis we're able to produce.\n",
    "#Remember to comment that our $ costs are \"approximations\" ... due to needing the baseprice assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slides Notes:\n",
    "#Maria is testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
